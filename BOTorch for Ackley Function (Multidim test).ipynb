{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6e66536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20-dimensional Optimization of Ackley Function using BOTorch\n",
    "\n",
    "import torch\n",
    "\n",
    "from gpytorch import kernels, means, likelihoods\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from pyDOE3 import lhs\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from scipy.interpolate import griddata\n",
    "\n",
    "from botorch.models.transforms import Standardize, Normalize\n",
    "#from botorch.utils.transforms import normalize, unnormalize\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.acquisition.logei import qLogNoisyExpectedImprovement\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "from botorch.test_functions import Ackley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63293948",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed =42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "noise_std = 0.5\n",
    "\n",
    "initial_sample = 24\n",
    "\n",
    "nu = 2.5\n",
    "\n",
    "dim = 10\n",
    "\n",
    "bounds = torch.stack([torch.zeros(dim), torch.ones(dim)])\n",
    "\n",
    "# standard Latin hypercube using maximin criterion (converts to tensor)\n",
    "lhs_design = torch.tensor(\n",
    "            lhs(n = dim, samples = initial_sample, criterion = 'maximin', random_state=seed), \n",
    "        dtype=torch.double\n",
    "        )\n",
    "\n",
    "#print(lhs_design, lhs_design.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4018612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Process Model\n",
    "\n",
    "# Only for further GP Model development\n",
    "\n",
    "# Setup (Assume train_X, train_Y, bounds, and objective_function are defined)\n",
    "# ===========================\n",
    "\n",
    "\n",
    "\n",
    "# GP Model definition\n",
    "class GPModel(SingleTaskGP):\n",
    "    def __init__(self, train_X, train_Y, fixed_noise=False,\n",
    "                 lengthscale_prior=None, outputscale_prior=None,\n",
    "                 lengthscale_constraint = None, outputscale_constraint=None):\n",
    "\n",
    "        if fixed_noise:\n",
    "            # set noise internally here if fixed_noise is True\n",
    "            noise_level = 0.1  # 10% of mean output\n",
    "            print(f\"Training with FIXED noise: {noise_level*100:.1f}% of mean output.\")\n",
    "            noise_variance = (noise_level * train_Y.mean()).pow(2)\n",
    "            train_Yvar = torch.full_like(train_Y, noise_variance)\n",
    "            likelihood = None\n",
    "            super().__init__(\n",
    "                train_X, train_Y, train_Yvar=train_Yvar, likelihood=likelihood,\n",
    "                outcome_transform=Standardize(m=1),\n",
    "                input_transform=Normalize(d=dim)\n",
    "            )\n",
    "        else:\n",
    "            print(\"Training with LEARNABLE noise (Gaussian Likelihood).\")\n",
    "            likelihood = likelihoods.GaussianLikelihood()\n",
    "            super().__init__(\n",
    "                train_X, train_Y, likelihood=likelihood,\n",
    "                outcome_transform=Standardize(m=1),\n",
    "                input_transform=Normalize(d=dim)\n",
    "            )\n",
    "            '''\n",
    "            lower_noise = lower_noise_bound**2  # lower noise bound\n",
    "            upper_noise = upper_noise_bound**2  # upper noise bound\n",
    "\n",
    "            # Add a **prior** (softly nudges during training)\n",
    "            self.likelihood.noise_covar.register_prior(\n",
    "                \"noise_prior\",\n",
    "                SmoothedBoxPrior(lower_noise, upper_noise),\n",
    "                \"raw_noise\"\n",
    "            )\n",
    "\n",
    "            # Add a **constraint** (hard bounding box)\n",
    "            self.likelihood.noise_covar.register_constraint(\n",
    "                \"raw_noise\",\n",
    "                Interval(lower_noise, upper_noise)\n",
    "            )\n",
    "            '''\n",
    "\n",
    "        self.mean_module = means.ConstantMean()\n",
    "\n",
    "        matern_kernel = kernels.MaternKernel(\n",
    "            nu=nu,\n",
    "            ard_num_dims=dim,\n",
    "            lengthscale_prior=lengthscale_prior,\n",
    "            lengthscale_constraint=lengthscale_constraint,\n",
    "        )\n",
    "\n",
    "        self.covar_module = kernels.ScaleKernel(\n",
    "            base_kernel=matern_kernel,\n",
    "            outputscale_prior=outputscale_prior,\n",
    "            outputscale_constraint=outputscale_constraint,\n",
    "        )\n",
    "        #self.likelihood=likelihood  # I added this to fix for fixed, but it might be redundant\n",
    "\n",
    "        \n",
    "# Training function\n",
    "def train_GP_model(train_X, train_Y, fixed_noise=False,\n",
    "                   lengthscale_prior=None, outputscale_prior=None,\n",
    "                   lengthscale_constraint = None, outputscale_constraint=None): \n",
    "    model = GPModel(\n",
    "        train_X, train_Y,\n",
    "        fixed_noise=fixed_noise,\n",
    "        \n",
    "        lengthscale_prior=lengthscale_prior,\n",
    "        outputscale_prior=outputscale_prior,\n",
    "        lengthscale_constraint = lengthscale_constraint, \n",
    "        outputscale_constraint=outputscale_constraint\n",
    "    )\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_mll(mll)\n",
    "    return model, mll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e04bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ===Iteration 1===\n",
      "Training with LEARNABLE noise (Gaussian Likelihood).\n",
      "Best Y: -2.1138 at X: tensor([0.4764, 0.2672, 0.0000, 0.0000, 0.1759, 0.7545, 0.0000, 0.1390, 0.3490,\n",
      "        0.2020], dtype=torch.float64)\n",
      "\n",
      " ===Iteration 2===\n",
      "Training with LEARNABLE noise (Gaussian Likelihood).\n",
      "Best Y: -1.9652 at X: tensor([0.0809, 0.3630, 0.0000, 0.0145, 0.0841, 0.6696, 0.0000, 0.1043, 0.5814,\n",
      "        0.7831], dtype=torch.float64)\n",
      "\n",
      " ===Iteration 3===\n",
      "Training with LEARNABLE noise (Gaussian Likelihood).\n",
      "Best Y: -1.5947 at X: tensor([0.0000, 0.4431, 0.0000, 0.9094, 0.0000, 0.1112, 0.0279, 0.0000, 0.2633,\n",
      "        0.3713], dtype=torch.float64)\n",
      "\n",
      " ===Iteration 4===\n",
      "Training with LEARNABLE noise (Gaussian Likelihood).\n",
      "Best Y: -1.5947 at X: tensor([0.0000, 0.4431, 0.0000, 0.9094, 0.0000, 0.1112, 0.0279, 0.0000, 0.2633,\n",
      "        0.3713], dtype=torch.float64)\n",
      "\n",
      " ===Iteration 5===\n",
      "Training with LEARNABLE noise (Gaussian Likelihood).\n",
      "Best Y: -1.5947 at X: tensor([0.0000, 0.4431, 0.0000, 0.9094, 0.0000, 0.1112, 0.0279, 0.0000, 0.2633,\n",
      "        0.3713], dtype=torch.float64)\n",
      "\n",
      " ===Iteration 6===\n",
      "Training with LEARNABLE noise (Gaussian Likelihood).\n",
      "Best Y: -1.5947 at X: tensor([0.0000, 0.4431, 0.0000, 0.9094, 0.0000, 0.1112, 0.0279, 0.0000, 0.2633,\n",
      "        0.3713], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 6\n",
    "num_restarts =50\n",
    "raw_samples =50\n",
    "batch_size = 20\n",
    "\n",
    "# the multidimensional function (where best answer is 0, where all X = 0)\n",
    "ackley = Ackley(dim=dim, noise_std = noise_std, negate=True) #negate = True to maximize the function\n",
    "\n",
    "train_X = lhs_design\n",
    "train_Y = ackley(train_X).unsqueeze(-1)\n",
    "\n",
    "#print(train_Y, train_Y.shape)\n",
    "\n",
    "for iterations in range (n_iterations):\n",
    "    print(f\"\\n ===Iteration {iterations+1}===\")\n",
    "    # Train the GP model\n",
    "    model, mll = train_GP_model(\n",
    "        train_X, train_Y,\n",
    "        fixed_noise=False,\n",
    "        #noise_level=noise_level,\n",
    "        lengthscale_prior=None,\n",
    "        outputscale_prior=None,\n",
    "        lengthscale_constraint = None, \n",
    "        outputscale_constraint=None\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    acq_func = qLogNoisyExpectedImprovement(model = model,\n",
    "                                            X_baseline=train_X,\n",
    "                                            prune_baseline = True, # default\n",
    "                                            cache_root = True, # default True (uses Cholesky decomposition)\n",
    "                                            )\n",
    "    \n",
    "    #Optimize acq func\n",
    "    candidate, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        q=batch_size,\n",
    "        bounds=bounds,\n",
    "        num_restarts=num_restarts,\n",
    "        raw_samples=raw_samples,\n",
    "    )\n",
    "\n",
    "    new_y = torch.stack([ackley(c.unsqueeze(0)) for c in candidate])\n",
    "\n",
    "    train_X = torch.cat([train_X, candidate], dim=0)\n",
    "    train_Y = torch.cat([train_Y, new_y], dim=0)\n",
    "\n",
    "    best_idx = torch.argmax(train_Y.squeeze(-1))\n",
    "    best_X = train_X[best_idx]\n",
    "    best_Y = train_Y[best_idx].item()\n",
    "    print(f\"Best Y: {best_Y:.4f} at X: {best_X}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f10915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Noise: 0.42491206791949265\n",
      "Mean Squared Error (MSE): 0.207977300923473\n",
      "R²: 0.5802982426280088\n",
      "Mean Absolute Percentage Error (MAPE): 11.477010265740093%\n",
      "True Function MSE: 0.06780214500027497\n",
      "True Function R²: 0.8110208656175081\n",
      "True Function MAPE: 6.398866031106294%\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(predictions, targets):\n",
    "    return torch.mean((predictions - targets) ** 2)\n",
    "\n",
    "def r_squared(predictions, targets):\n",
    "    ss_total = torch.sum((targets - torch.mean(targets)) ** 2)\n",
    "    ss_residual = torch.sum((targets - predictions) ** 2)\n",
    "    return 1 - ss_residual / ss_total\n",
    "\n",
    "def mape_loss(predictions, targets):\n",
    "    return torch.mean(torch.abs((targets - predictions) / targets +1e-8)) * 100\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    posterior_test = model.posterior(train_X)\n",
    "    mean_test = posterior_test.mean\n",
    "    stddev = posterior_test.variance.sqrt()\n",
    "\n",
    "mse = mse_loss(mean_test, train_Y)\n",
    "r2 = r_squared(mean_test, train_Y)\n",
    "mape = mape_loss(mean_test, train_Y)\n",
    "\n",
    "# print learned noise\n",
    "print(\"Learned Noise:\", model.likelihood.noise_covar.noise.max().item())\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Squared Error (MSE): {mse.item()}\")\n",
    "print(f\"R²: {r2.item()}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape.item()}%\")\n",
    "\n",
    "true_Y = Ackley(dim = dim, noise_std =0, negate= True)  # True function without noise\n",
    "true_Y = true_Y(train_X).unsqueeze(-1)  # Reshape to (n, 1)\n",
    "mse_true = mse_loss(mean_test, true_Y)\n",
    "r2_true = r_squared(mean_test, true_Y)\n",
    "mape_true = mape_loss(mean_test, true_Y)\n",
    "\n",
    "# Print the results\n",
    "print(f\"True Function MSE: {mse_true.item()}\")\n",
    "print(f\"True Function R²: {r2_true.item()}\")\n",
    "print(f\"True Function MAPE: {mape_true.item()}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
