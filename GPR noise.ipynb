{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a general page for testing differnt types of GPyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom GP model with noise (as a template example of simplest custom model)\n",
    "\n",
    "This one could be useful to illustrate how to add confidence bound in the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 3.078855514526367\n",
      "Iter 11/100 - Loss: 1.9717966318130493\n",
      "Iter 21/100 - Loss: 1.71285080909729\n",
      "Iter 31/100 - Loss: 1.6187007427215576\n",
      "Iter 41/100 - Loss: 1.5633478164672852\n",
      "Iter 51/100 - Loss: 1.5184382200241089\n",
      "Iter 61/100 - Loss: 1.4958021640777588\n",
      "Iter 71/100 - Loss: 1.4945569038391113\n",
      "Iter 81/100 - Loss: 1.4927973747253418\n",
      "Iter 91/100 - Loss: 1.4926453828811646\n",
      "Mean predictions: tensor([1.6084, 2.5340, 3.5343])\n",
      "Confidence region: tensor([0.8906, 1.8222, 2.8165]) to tensor([2.3262, 3.2457, 4.2521])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic Gaussian Process Regression with GPyTorch NOISE!!!!\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Define the model class\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the GP forward pass\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "\n",
    "# Training data\n",
    "train_x = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)\n",
    "train_y = torch.tensor([1.2, 2.1, 3.0, 4.2], dtype=torch.float32)\n",
    "train_y_noisy = train_y + torch.normal(0, 0.1, size=train_y.shape)  # Add noise\n",
    "\n",
    "# Likelihood models the observation noise\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# Initialize the model\n",
    "model = GPModel(train_x, train_y_noisy, likelihood)\n",
    "\n",
    "# Set the model into training mode\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  \n",
    "], lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# Training loop\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y_noisy)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iter {i + 1}/100 - Loss: {loss.item()}')\n",
    "\n",
    "# After training, set the model into evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions on test data\n",
    "test_x = torch.tensor([[1.5], [2.5], [3.5]], dtype=torch.float32)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    pred = model(test_x)\n",
    "    mean = pred.mean\n",
    "    lower, upper = pred.confidence_region()\n",
    "\n",
    "# Print results\n",
    "print(f'Mean predictions: {mean}')\n",
    "print(f'Confidence region: {lower} to {upper}')\n",
    "\n",
    "# Plot results\n",
    "plt.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "plt.plot(test_x.numpy(), mean.numpy(), 'r')\n",
    "plt.fill_between(test_x.numpy().flatten(), lower.numpy().flatten(), upper.numpy().flatten(), color='pink', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections is a tutorial on gpytorch hyperparameters from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding gpytorch hyperparameters (uses import from prev cell)\n",
    "import gpytorch\n",
    "import torch\n",
    "import math\n",
    "\n",
    "train_x = torch.linspace(0, 1, 100) # 100 points between 0 and 1\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2 # Noisy sine wave\n",
    "\n",
    "\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "#initialize likelihood and model\n",
    "likelihood  = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: likelihood.noise_covar.raw_noise           value = 0.0\n",
      "Parameter name: mean_module.raw_constant                   value = 0.0\n",
      "Parameter name: covar_module.raw_outputscale               value = 0.0\n",
      "Parameter name: covar_module.base_kernel.raw_lengthscale   value = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Let's see what the parameters of this model are (These should be 0s since we haven't trained yet)\n",
    "\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f'Parameter name: {param_name:42} value = {param.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw outputscale,  Parameter containing:\n",
      "tensor(0., requires_grad=True)\n",
      "\n",
      "raw_outputscale_constraint1 Positive()\n",
      "\n",
      "printing all model constraints:\n",
      "Constraint name: likelihood.noise_covar.raw_noise_constraint             value = GreaterThan(1.000E-04)\n",
      "Constraint name: covar_module.raw_outputscale_constraint                 value = Positive()\n",
      "Constraint name: covar_module.base_kernel.raw_lengthscale_constraint     value = Positive()\n",
      "\n",
      "getting raw outputscale constraint from model.covar_module...\n",
      "Positive()\n"
     ]
    }
   ],
   "source": [
    "#actual learned parameters of the model are things like noise and lengthscale, which must be positive \n",
    "# let's look at the parameter raw_outputscale\n",
    "raw_outputscale = model.covar_module.raw_outputscale\n",
    "print('raw outputscale, ', raw_outputscale)\n",
    "\n",
    "# How can we look at the constraints ( as we want to make sure they are positive)\n",
    "# three different ways\n",
    "\n",
    "#1\n",
    "print('\\nraw_outputscale_constraint1', model.covar_module.raw_outputscale_constraint)\n",
    "#2\n",
    "print('\\nprinting all model constraints:')\n",
    "for constraint_name, constraint in model.named_constraints():\n",
    "    print(f'Constraint name: {constraint_name:55} value = {constraint}')\n",
    "#3\n",
    "print('\\ngetting raw outputscale constraint from model.covar_module...')\n",
    "print(model.covar_module.constraint_for_parameter_name('raw_outputscale')) # this is the same as the first one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed ouputscale tensor(0.6931, grad_fn=<SoftplusBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "True\n",
      "Transform a bunch of negative tensors:  tensor([0.3133, 0.1269, 0.0486])\n"
     ]
    }
   ],
   "source": [
    "# These constraints define 'transform' and 'inverse_transform', which turns raw parameters\n",
    "# into real ones. For a positive constraint, we expect the transformed values to always be positive\n",
    "raw_outputscale = model.covar_module.raw_outputscale\n",
    "constraint = model.covar_module.raw_outputscale_constraint\n",
    "\n",
    "print('Transformed ouputscale', constraint.transform(raw_outputscale))\n",
    "print(constraint.inverse_transform(constraint.transform(raw_outputscale))) # should be same since trans than intrans\n",
    "print(torch.equal(constraint.inverse_transform(constraint.transform(raw_outputscale)), raw_outputscale)) # returns True\n",
    "\n",
    "print('Transform a bunch of negative tensors: ', constraint.transform(torch.tensor([-1.0, -2.0, -3.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual outputscale:  tensor(0.6931, grad_fn=<SoftplusBackward0>)\n",
      "Actual outputscale:  tensor(2., grad_fn=<SoftplusBackward0>)\n",
      "outputscale tensor(0.6931, grad_fn=<SoftplusBackward0>)\n",
      "outputscale tensor(3., grad_fn=<SoftplusBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# obviously, raw parameter values are difficult to work with. Humans understand things like noise variance = 0.01\n",
    "# but we struggle to understand what raw_noise = -2.791 might mean. \n",
    "# all gpytorch modules that define raw parameters define convenience getters and setters for dealing with\n",
    "# transformed values directly.\n",
    "\n",
    "#reset model\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "#getting\n",
    "print('Actual outputscale: ', model.covar_module.outputscale)\n",
    "#setting\n",
    "model.covar_module.outputscale = 2.\n",
    "print('Actual outputscale: ', model.covar_module.outputscale)\n",
    "\n",
    "#reset model\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "# these are shortcuts to do it the long way which would look like this \n",
    "raw_outputscale = model.covar_module.raw_outputscale\n",
    "constraint = model.covar_module.raw_outputscale_constraint\n",
    "outputscale = constraint.transform(raw_outputscale)\n",
    "print('outputscale', outputscale)\n",
    "\n",
    "model.covar_module.raw_outputscale.data.fill_(constraint.inverse_transform(torch.tensor(2.0)))\n",
    "raw_outputscale = model.covar_module.raw_outputscale\n",
    "outputscale = constraint.transform(raw_outputscale)\n",
    "print('outputscale', outputscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual noise value:  tensor([0.6932], grad_fn=<AddBackward0>)\n",
      "Noise constraint:  GreaterThan(1.000E-04)\n"
     ]
    }
   ],
   "source": [
    "# noise\n",
    "print('Actual noise value: ', likelihood.noise)\n",
    "print('Noise constraint: ', likelihood.noise_covar.raw_noise_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise constraint:  GreaterThan(1.000E-01)\n",
      "noise constraint:  Positive()\n"
     ]
    }
   ],
   "source": [
    "# noise can be set when created\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(0.1))\n",
    "print('noise constraint: ', likelihood.noise_covar.raw_noise_constraint)\n",
    "\n",
    "# or changed after module is created\n",
    "likelihood.noise_covar.register_constraint('raw_noise', gpytorch.constraints.Positive())\n",
    "print('noise constraint: ', likelihood.noise_covar.raw_noise_constraint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priors can also be or created\n",
    "# registers a prior on the sqrt of the noise parameter (e.g. a prior for the nosie SD instead of var)\n",
    "likelihood.noise_covar.register_prior(\n",
    "    'noise_std_prior',\n",
    "    gpytorch.priors.NormalPrior(0,1), # noraml distribution with prior mean 0 and std 1\n",
    "    lambda module: module.noise.sqrt() # this is the bit that modifies the prior (by sqrt'ing the noise parameter)\n",
    ")\n",
    "\n",
    "# Create a Gaussianlikelihood with a normal prior for the noise\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "    noise_constraint = gpytorch.constraints.GreaterThan(1e-3),\n",
    "    noise_prior = gpytorch.priors.NormalPrior(0,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's follow the tutorial and make a custom gp using Exact GP\n",
    "# this provides an example of the above all put together to customize a the ExactGP model\n",
    "\n",
    "class FancyGPWithPriors(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(FancyGPWithPriors, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        lengthscale_prior = gpytorch.priors.GammaPrior(3.0, 6.0) # gamma prior with shape 3 and scale 6\n",
    "        outputscale_prior = gpytorch.priors.GammaPrior(2.0, 0.15) # gamma prior with shape 2 and scale 0.15\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                lengthscale_prior=lengthscale_prior,\n",
    "            ),\n",
    "            outputscale_prior = outputscale_prior\n",
    "        )\n",
    "\n",
    "        # Initialize lengthscale and outputscale to mean of priors\n",
    "        self.covar_module.base_kernel.lengthscale = lengthscale_prior.mean\n",
    "        self.covar_module.outputscale = outputscale_prior.mean\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(\n",
    "    noise_constraint = gpytorch.constraints.GreaterThan(1e-2)\n",
    ")\n",
    "\n",
    "model = FancyGPWithPriors(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000001192092896 0.5 2.0\n"
     ]
    }
   ],
   "source": [
    "# and initializing hyperparameters in one call \n",
    "# for convenience, gpytorch modules allow to update a full dictionary of parametes on submodules. for example:\n",
    "\n",
    "hypers = {\n",
    "    'likelihood.noise_covar.noise': torch.tensor(1.),\n",
    "    'covar_module.base_kernel.lengthscale': torch.tensor(0.5),\n",
    "    'covar_module.outputscale': torch.tensor(2.)\n",
    "}\n",
    "\n",
    "model.initialize(**hypers) # this will set the values of the parameters in the model to the values in the hypers dictionary\n",
    "print(\n",
    "    model.likelihood.noise_covar.noise.item(),\n",
    "    model.covar_module.base_kernel.lengthscale.item(),\n",
    "    model.covar_module.outputscale.item()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to save and load models in gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is exact same from the first cell of the hyperparameters section\n",
    "\n",
    "train_x = torch.linspace(0, 1, 100) # 100 points between 0 and 1\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2 # Noisy sine wave\n",
    "\n",
    "\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "#initialize likelihood and model\n",
    "likelihood  = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's change default hyperparameters to demonstrate how this works\n",
    "model.covar_module.outputscale = 2.3\n",
    "model.covar_module.base_kernel.lengthscale = 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('likelihood.noise_covar.raw_noise', tensor([0.])),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.lower_bound',\n",
       "              tensor(1.0000e-04)),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('mean_module.raw_constant', tensor(0.)),\n",
       "             ('covar_module.raw_outputscale', tensor(2.1944)),\n",
       "             ('covar_module.base_kernel.raw_lengthscale', tensor([[6.1980]])),\n",
       "             ('covar_module.base_kernel.raw_lengthscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.base_kernel.raw_lengthscale_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('covar_module.raw_outputscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.raw_outputscale_constraint.upper_bound',\n",
       "              tensor(inf))])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will call the full state of the gpytorch model\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this will save that\n",
    "torch.save(model.state_dict(), 'model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'likelihood.noise_covar.raw_noise': tensor([0.]), 'likelihood.noise_covar.raw_noise_constraint.lower_bound': tensor(1.0000e-04), 'likelihood.noise_covar.raw_noise_constraint.upper_bound': tensor(inf), 'mean_module.raw_constant': tensor(0.), 'covar_module.raw_outputscale': tensor(0.), 'covar_module.base_kernel.raw_lengthscale': tensor([[0.]]), 'covar_module.base_kernel.raw_lengthscale_constraint.lower_bound': tensor(0.), 'covar_module.base_kernel.raw_lengthscale_constraint.upper_bound': tensor(inf), 'covar_module.raw_outputscale_constraint.lower_bound': tensor(0.), 'covar_module.raw_outputscale_constraint.upper_bound': tensor(inf)})\n",
      "OrderedDict({'likelihood.noise_covar.raw_noise': tensor([0.]), 'likelihood.noise_covar.raw_noise_constraint.lower_bound': tensor(1.0000e-04), 'likelihood.noise_covar.raw_noise_constraint.upper_bound': tensor(inf), 'mean_module.raw_constant': tensor(0.), 'covar_module.raw_outputscale': tensor(2.1944), 'covar_module.base_kernel.raw_lengthscale': tensor([[6.1980]]), 'covar_module.base_kernel.raw_lengthscale_constraint.lower_bound': tensor(0.), 'covar_module.base_kernel.raw_lengthscale_constraint.upper_bound': tensor(inf), 'covar_module.raw_outputscale_constraint.lower_bound': tensor(0.), 'covar_module.raw_outputscale_constraint.upper_bound': tensor(inf)})\n"
     ]
    }
   ],
   "source": [
    "# let's load this state (with a demonstration to show it makes a difference)\n",
    "state_dict = torch.load('model_state.pth')\n",
    "model=ExactGPModel(train_x, train_y, likelihood) # reinitialize a new model\n",
    "\n",
    "before = model.state_dict()\n",
    "print(before)\n",
    "\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "after = model.state_dict()\n",
    "\n",
    "print(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an advanced example with a DKL (Deep Kernel Learning model)\n",
    "# this model type allows the NN to transform raw input into a better space for GP to operate on (similar to preprocessing)\n",
    "# basically the GPR is operating over the NN output\n",
    "# the whole model can be optimized using this gramework under marginal log likelihood\n",
    "\n",
    "class GPwithNNFeatureExtractor(gpytorch.models.ExactGP): # custom GP model \n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPwithNNFeatureExtractor, self).__init__(train_x, train_y, likelihood) # this and prev line calls the parent class with training data and likelihood = Gaussian noise model\n",
    "        self.mean_module = gpytorch.means.ConstantMean() # assumes the output has a constant mean (learnable scalar)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) # ScaleKernel wraps the RBF kernel with a scalar multiplier\n",
    "\n",
    "        self.feature_extractor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, 2),\n",
    "            torch.nn.BatchNorm1d(2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2, 2),\n",
    "            torch.nn.BatchNorm1d(2),\n",
    "            torch.nn.ReLU(),\n",
    "        ) # simple 2-layer feed-forward (1 input, 2 transformed features, with Batch norm to stabilize training, and ReLU adding non-linearity)\n",
    "\n",
    "    def forward(self, x): # even though we define the nn architecture last, we see below it is executed first before being fed into the GPR\n",
    "        x = self.feature_extractor(x)\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "lieklihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPwithNNFeatureExtractor(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('likelihood.noise_covar.raw_noise', tensor([0.])),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.lower_bound',\n",
       "              tensor(1.0000e-04)),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('mean_module.raw_constant', tensor(0.)),\n",
       "             ('covar_module.raw_outputscale', tensor(0.)),\n",
       "             ('covar_module.base_kernel.raw_lengthscale', tensor([[0.]])),\n",
       "             ('covar_module.base_kernel.raw_lengthscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.base_kernel.raw_lengthscale_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('covar_module.raw_outputscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.raw_outputscale_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('feature_extractor.0.weight',\n",
       "              tensor([[0.8996],\n",
       "                      [0.2294]])),\n",
       "             ('feature_extractor.0.bias', tensor([-0.3118, -0.3235])),\n",
       "             ('feature_extractor.1.weight', tensor([1., 1.])),\n",
       "             ('feature_extractor.1.bias', tensor([0., 0.])),\n",
       "             ('feature_extractor.1.running_mean', tensor([0., 0.])),\n",
       "             ('feature_extractor.1.running_var', tensor([1., 1.])),\n",
       "             ('feature_extractor.1.num_batches_tracked', tensor(0)),\n",
       "             ('feature_extractor.3.weight',\n",
       "              tensor([[-0.5799,  0.3661],\n",
       "                      [-0.4761,  0.2540]])),\n",
       "             ('feature_extractor.3.bias', tensor([0.1237, 0.5527])),\n",
       "             ('feature_extractor.4.weight', tensor([1., 1.])),\n",
       "             ('feature_extractor.4.bias', tensor([0., 0.])),\n",
       "             ('feature_extractor.4.running_mean', tensor([0., 0.])),\n",
       "             ('feature_extractor.4.running_var', tensor([1., 1.])),\n",
       "             ('feature_extractor.4.num_batches_tracked', tensor(0))])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict() # this will show the orderdictionary which is much more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# however saving and loading will work exactly the same\n",
    "torch.save(model.state_dict(), 'my_gp_with_nn_model.pth')\n",
    "state_dict = torch.load('my_gp_with_nn_model.pth')\n",
    "model = GPwithNNFeatureExtractor(train_x, train_y, likelihood)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('likelihood.noise_covar.raw_noise', tensor([0.])),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.lower_bound',\n",
       "              tensor(1.0000e-04)),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('mean_module.raw_constant', tensor(0.)),\n",
       "             ('covar_module.raw_outputscale', tensor(0.)),\n",
       "             ('covar_module.base_kernel.raw_lengthscale', tensor([[0.]])),\n",
       "             ('covar_module.base_kernel.raw_lengthscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.base_kernel.raw_lengthscale_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('covar_module.raw_outputscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.raw_outputscale_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('feature_extractor.0.weight',\n",
       "              tensor([[0.8996],\n",
       "                      [0.2294]])),\n",
       "             ('feature_extractor.0.bias', tensor([-0.3118, -0.3235])),\n",
       "             ('feature_extractor.1.weight', tensor([1., 1.])),\n",
       "             ('feature_extractor.1.bias', tensor([0., 0.])),\n",
       "             ('feature_extractor.1.running_mean', tensor([0., 0.])),\n",
       "             ('feature_extractor.1.running_var', tensor([1., 1.])),\n",
       "             ('feature_extractor.1.num_batches_tracked', tensor(0)),\n",
       "             ('feature_extractor.3.weight',\n",
       "              tensor([[-0.5799,  0.3661],\n",
       "                      [-0.4761,  0.2540]])),\n",
       "             ('feature_extractor.3.bias', tensor([0.1237, 0.5527])),\n",
       "             ('feature_extractor.4.weight', tensor([1., 1.])),\n",
       "             ('feature_extractor.4.bias', tensor([0., 0.])),\n",
       "             ('feature_extractor.4.running_mean', tensor([0., 0.])),\n",
       "             ('feature_extractor.4.running_var', tensor([1., 1.])),\n",
       "             ('feature_extractor.4.num_batches_tracked', tensor(0))])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is for Kernels: Sum and Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base kernels\n",
    "rbf_kernel_1 = gpytorch.kernels.RBFKernel()\n",
    "cos_kernel_1 = gpytorch.kernels.CosineKernel()\n",
    "rbf_kernel_2 = gpytorch.kernels.RBFKernel()\n",
    "cos_kernel_2 = gpytorch.kernels.CosineKernel()\n",
    "\n",
    "# Manual Implementation \n",
    "spectral_mixture_kernel_1 = (rbf_kernel_1 * cos_kernel_1) + (rbf_kernel_2 * cos_kernel_2)\n",
    "# of course, it probably makes more sense to use the spectral mixture kernel provided in gpytorch\n",
    "spectral_mixture_kernel_2 = gpytorch.kernels.SpectralMixtureKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# efficient summation of univariate kernels\n",
    "d=3\n",
    "\n",
    "batch_univariate_rbf_kernel = gpytorch.kernels.RBFKernel(\n",
    "    batch_shape = torch.Size([d]), # A batch of d...\n",
    "    ard_num_dims=1 # ...univariate kernels\n",
    ")\n",
    "\n",
    "#including batch_shape ensure that the lengthscale parameter of batch_univariate_rbf_kernel is a d x 1 x 1 tensor\n",
    "# meaning each univariate kernel will have its own lengthscale (you can have them all have the same lengthscale by omitting the batch_shape argument)\n",
    "\n",
    "\n",
    "\n",
    "#to compute the univariate kernel matrices, we need to feed the appropriate dimensions of X into each of the component\n",
    "# kernels. We accomplish this by reshaping the 'n x d' matrix representing X into a batch of d x (n x 1) \n",
    "n = 10\n",
    "\n",
    "X = torch.randn(n, d) # some random data in a n x d matrix\n",
    "batched_dimensions_of_X = X.mT.unsqueeze(-1) # now a 'd x n x 1' tensor\n",
    "\n",
    "# we then feed the batches of univariate data into the batched kernel object to get our batch of univariate kernel matrices\n",
    "univariate_rbf_covars = batch_univariate_rbf_kernel(batched_dimensions_of_X)\n",
    "univariate_rbf_covars.shape # d x n x n\n",
    "\n",
    "# finally computing the sum over the batch (sum of univariate kernels)\n",
    "additive_covar = univariate_rbf_covars.sum(dim=-3) # this could just be dim = 0 or no argument for same results\n",
    "additive_covar.shape # n x n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x00000251CC1B3CE0>\n",
      "naive_additive_kernel(X)\n",
      "  4.52 ms\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x00000251CC4FB1A0>\n",
      "batch_univariate_rbf_kernel(X.mT.unsqueeze(-1)).sum(dim=-3)\n",
      "  5.84 ms\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "# the followin is to demonstrate faster processes on GPU, which of course doesn't make a difference on my computer\n",
    "from torch. utils import benchmark\n",
    "\n",
    "d = 10\n",
    "n = 500\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "X = torch.randn(n, d, device=device)\n",
    "\n",
    "naive_additive_kernel = (\n",
    "    gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=[0]) +\n",
    "    gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=[1]) +\n",
    "    gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=[2]) +\n",
    "    gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=[3]) +\n",
    "    gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=[4]) +\n",
    "    gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=[5]) +\n",
    "    gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=[6]) +\n",
    "    gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=[7]) +\n",
    "    gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=[8]) +\n",
    "    gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=[9])\n",
    ").to(device=device)\n",
    "\n",
    "with gpytorch.settings.lazily_evaluate_kernels(False):\n",
    "    print(benchmark.Timer(\n",
    "        stmt=\"naive_additive_kernel(X)\",\n",
    "        globals={\"naive_additive_kernel\": naive_additive_kernel, \"X\": X}\n",
    "    ).timeit(100))\n",
    "\n",
    "# using the variable (same as above example) the code below should be significantly faster on GPU\n",
    "batch_univariate_rbf_kernel = gpytorch.kernels.RBFKernel(\n",
    "    batch_shape=torch.Size([d]), ard_num_dims=1,\n",
    ").to(device=device)\n",
    "\n",
    "with gpytorch.settings.lazily_evaluate_kernels(False):\n",
    "    print(benchmark.Timer(\n",
    "        stmt=\"batch_univariate_rbf_kernel(X.mT.unsqueeze(-1)).sum(dim=-3)\",\n",
    "        globals={\"batch_univariate_rbf_kernel\": batch_univariate_rbf_kernel, \"X\": X}\n",
    "    ).timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a full example using GP wit additive kernels\n",
    "\n",
    "class AdditiveKernelGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, X_train, y_train, d):\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        super().__init__(X_train, y_train, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape = torch.Size([d]), \n",
    "                                       ard_num_dims = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = self.mean_module(X)\n",
    "        batched_dimensions_of_X = X.mT.unsqueeze(-1) # now a d x n x 1 tensor\n",
    "        covar = self.covar_module(batched_dimensions_of_X).sum(dim=-3)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll just add a standard base model here, which should be sufficient for general scientific purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.5816723108291626\n",
      "Iteration 1, Loss: 1.5640580654144287\n",
      "Iteration 2, Loss: 1.552570104598999\n",
      "Iteration 3, Loss: 1.5462846755981445\n",
      "Iteration 4, Loss: 1.5437391996383667\n",
      "Iteration 5, Loss: 1.5432567596435547\n",
      "Iteration 6, Loss: 1.5436580181121826\n",
      "Iteration 7, Loss: 1.5443774461746216\n",
      "Iteration 8, Loss: 1.5451809167861938\n",
      "Iteration 9, Loss: 1.5459697246551514\n",
      "Iteration 10, Loss: 1.5466740131378174\n",
      "Iteration 11, Loss: 1.5471903085708618\n",
      "Iteration 12, Loss: 1.5473798513412476\n",
      "Iteration 13, Loss: 1.5471376180648804\n",
      "Iteration 14, Loss: 1.5464680194854736\n",
      "Iteration 15, Loss: 1.5454977750778198\n",
      "Iteration 16, Loss: 1.5444262027740479\n",
      "Iteration 17, Loss: 1.543452262878418\n",
      "Iteration 18, Loss: 1.5427170991897583\n",
      "Iteration 19, Loss: 1.5422743558883667\n",
      "Iteration 20, Loss: 1.542096495628357\n",
      "Iteration 21, Loss: 1.5421099662780762\n",
      "Iteration 22, Loss: 1.5422348976135254\n",
      "Iteration 23, Loss: 1.5424083471298218\n",
      "Iteration 24, Loss: 1.5425866842269897\n",
      "Iteration 25, Loss: 1.542739987373352\n",
      "Iteration 26, Loss: 1.5428427457809448\n",
      "Iteration 27, Loss: 1.5428712368011475\n",
      "Iteration 28, Loss: 1.5428084135055542\n",
      "Iteration 29, Loss: 1.5426535606384277\n",
      "Iteration 30, Loss: 1.5424305200576782\n",
      "Iteration 31, Loss: 1.5421855449676514\n",
      "Iteration 32, Loss: 1.5419732332229614\n",
      "Iteration 33, Loss: 1.5418370962142944\n",
      "Iteration 34, Loss: 1.5417944192886353\n",
      "Iteration 35, Loss: 1.5418317317962646\n",
      "Iteration 36, Loss: 1.5419129133224487\n",
      "Iteration 37, Loss: 1.5419960021972656\n",
      "Iteration 38, Loss: 1.5420504808425903\n",
      "Iteration 39, Loss: 1.5420650243759155\n",
      "Iteration 40, Loss: 1.542046308517456\n",
      "Iteration 41, Loss: 1.542008638381958\n",
      "Iteration 42, Loss: 1.5419639348983765\n",
      "Iteration 43, Loss: 1.5419182777404785\n",
      "Iteration 44, Loss: 1.5418729782104492\n",
      "Iteration 45, Loss: 1.5418295860290527\n",
      "Iteration 46, Loss: 1.5417935848236084\n",
      "Iteration 47, Loss: 1.541772723197937\n",
      "Iteration 48, Loss: 1.5417717695236206\n",
      "Iteration 49, Loss: 1.5417883396148682\n",
      "Iteration 50, Loss: 1.5418128967285156\n",
      "Iteration 51, Loss: 1.5418332815170288\n",
      "Iteration 52, Loss: 1.5418403148651123\n",
      "Iteration 53, Loss: 1.5418322086334229\n",
      "Iteration 54, Loss: 1.541813611984253\n",
      "Iteration 55, Loss: 1.5417931079864502\n",
      "Iteration 56, Loss: 1.5417762994766235\n",
      "Iteration 57, Loss: 1.5417659282684326\n",
      "Iteration 58, Loss: 1.5417611598968506\n",
      "Iteration 59, Loss: 1.541759729385376\n",
      "Iteration 60, Loss: 1.5417605638504028\n",
      "Iteration 61, Loss: 1.5417627096176147\n",
      "Iteration 62, Loss: 1.54176664352417\n",
      "Iteration 63, Loss: 1.5417706966400146\n",
      "Iteration 64, Loss: 1.5417726039886475\n",
      "Iteration 65, Loss: 1.5417712926864624\n",
      "Iteration 66, Loss: 1.5417660474777222\n",
      "Iteration 67, Loss: 1.5417591333389282\n",
      "Iteration 68, Loss: 1.5417530536651611\n",
      "Iteration 69, Loss: 1.5417495965957642\n",
      "Iteration 70, Loss: 1.5417492389678955\n",
      "Iteration 71, Loss: 1.5417506694793701\n",
      "Iteration 72, Loss: 1.5417524576187134\n",
      "Iteration 73, Loss: 1.5417536497116089\n",
      "Iteration 74, Loss: 1.5417540073394775\n",
      "Iteration 75, Loss: 1.5417537689208984\n",
      "Iteration 76, Loss: 1.5417531728744507\n",
      "Iteration 77, Loss: 1.5417520999908447\n",
      "Iteration 78, Loss: 1.5417505502700806\n",
      "Iteration 79, Loss: 1.5417487621307373\n",
      "Iteration 80, Loss: 1.5417473316192627\n",
      "Iteration 81, Loss: 1.5417468547821045\n",
      "Iteration 82, Loss: 1.5417472124099731\n",
      "Iteration 83, Loss: 1.5417481660842896\n",
      "Iteration 84, Loss: 1.5417488813400269\n",
      "Iteration 85, Loss: 1.5417492389678955\n",
      "Iteration 86, Loss: 1.5417487621307373\n",
      "Iteration 87, Loss: 1.5417481660842896\n",
      "Iteration 88, Loss: 1.5417474508285522\n",
      "Iteration 89, Loss: 1.541746973991394\n",
      "Iteration 90, Loss: 1.5417468547821045\n",
      "Iteration 91, Loss: 1.5417464971542358\n",
      "Iteration 92, Loss: 1.5417464971542358\n",
      "Iteration 93, Loss: 1.5417464971542358\n",
      "Iteration 94, Loss: 1.5417468547821045\n",
      "Iteration 95, Loss: 1.541746973991394\n",
      "Iteration 96, Loss: 1.5417470932006836\n",
      "Iteration 97, Loss: 1.541746973991394\n",
      "Iteration 98, Loss: 1.5417466163635254\n",
      "Iteration 99, Loss: 1.5417463779449463\n",
      "Learned noise level: 0.9306204915046692\n"
     ]
    }
   ],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Example toy data: 10 samples with some noise added to the target values\n",
    "train_x = torch.randn(10, 1)\n",
    "train_y = torch.randn(10) + 0.1 * torch.randn(10)  # Add some noise\n",
    "\n",
    "# Define the Gaussian likelihood (with noise)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# Define the GP model (Exact GP)\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize the model\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "# Define the marginal log-likelihood (MLL) function\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# Optimizer for training the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)  # Minimize the negative log-likelihood\n",
    "    print(f\"Iteration {i}, Loss: {loss.item()}\")  # .item() converts to a scalar value for printing\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training, you can check the learned noise level\n",
    "print(\"Learned noise level:\", likelihood.noise.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
