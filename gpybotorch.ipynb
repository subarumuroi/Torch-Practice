{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.ones_like(x)  # Creates a tensor of ones with the same shape as x\n",
    "\n",
    "# Basic tensor operations\n",
    "z = x + y  # Element-wise addition\n",
    "w = torch.matmul(x, y)  # Matrix multiplication (dot product if 1D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4.])\n"
     ]
    }
   ],
   "source": [
    "# Example with autograd\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = x**2\n",
    "z = y.sum()\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(x.grad)  # Gradient of z with respect to x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 5)  # Input: 10 features, Output: 5 features\n",
    "        self.layer2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Model and optimizer\n",
    "model = SimpleNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Zero the gradients, perform a backward pass, and update weights\n",
    "optimizer.zero_grad()  # Clears old gradients\n",
    "output = model(torch.randn(1, 10))  # Example input\n",
    "loss = output.sum()  # Just an example loss function\n",
    "loss.backward()\n",
    "optimizer.step()  # Updates parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Training a GP model with GPytorch involves defining a likelihood and optimizing the model parameters using PyTorch's optimizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/_4/f_rgwft14s14_j1xnpz_zn7c0000gn/T/ipykernel_12979/3905559278.py:10: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  gp = SingleTaskGP(train_x, train_y)\n",
      "/Users/s/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/botorch/acquisition/analytic.py:331: NumericsWarning: ExpectedImprovement has known numerical issues that lead to suboptimal optimization performance. It is strongly recommended to simply replace\n",
      "\n",
      "\t ExpectedImprovement \t --> \t LogExpectedImprovement \n",
      "\n",
      "instead, which fixes the issues and has the same API. See https://arxiv.org/abs/2310.20708 for details.\n",
      "  legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "optimize_acqf() missing 1 required positional argument: 'q'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbotorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize_acqf\n\u001b[32m     17\u001b[39m acqf = ExpectedImprovement(gp, best_f=train_y.max())\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m next_point, _ = \u001b[43moptimize_acqf\u001b[49m\u001b[43m(\u001b[49m\u001b[43macqf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_restarts\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: optimize_acqf() missing 1 required positional argument: 'q'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "# Define some data points and a GP model\n",
    "train_x = torch.rand(10, 1)  # 10 data points, 1 feature each\n",
    "train_y = torch.sin(train_x)\n",
    "\n",
    "# Build a single-task GP model\n",
    "gp = SingleTaskGP(train_x, train_y)\n",
    "\n",
    "# Define an acquisition function (e.g., Expected Improvement)\n",
    "# and optimize it to find the next sample\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "acqf = ExpectedImprovement(gp, best_f=train_y.max())\n",
    "next_point, _ = optimize_acqf(acqf, bounds=torch.tensor([[0.], [1.]]), num_restarts=5, raw_samples=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/f_rgwft14s14_j1xnpz_zn7c0000gn/T/ipykernel_13358/1056962898.py:11: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  gp = SingleTaskGP(train_x, train_y)\n",
      "/Users/s/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/botorch/acquisition/analytic.py:331: NumericsWarning: ExpectedImprovement has known numerical issues that lead to suboptimal optimization performance. It is strongly recommended to simply replace\n",
      "\n",
      "\t ExpectedImprovement \t --> \t LogExpectedImprovement \n",
      "\n",
      "instead, which fixes the issues and has the same API. See https://arxiv.org/abs/2310.20708 for details.\n",
      "  legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "# Define some data points and a GP model\n",
    "train_x = torch.rand(10, 1)  # 10 data points, 1 feature each\n",
    "train_y = torch.sin(train_x)\n",
    "\n",
    "# Build a single-task GP model\n",
    "gp = SingleTaskGP(train_x, train_y)\n",
    "\n",
    "# Define an acquisition function (e.g., Expected Improvement)\n",
    "acqf = ExpectedImprovement(gp, best_f=train_y.max())\n",
    "\n",
    "# Specify the number of candidates to optimize over\n",
    "q = 1  # For single candidate optimization\n",
    "\n",
    "#define boudnds\n",
    "bounds = torch.tensor([[0.],[1.]])\n",
    "\n",
    "# Optimize the acquisition function to find the next sample\n",
    "next_point, _ = optimize_acqf(\n",
    "    acqf, \n",
    "    bounds=bounds,  #pass\n",
    "    num_restarts=5,  # Number of random restarts for optimization\n",
    "    raw_samples=20,  # Number of raw candidates to sample before optimization\n",
    "    q=q  # Number of candidates to optimize over at once\n",
    ")\n",
    "\n",
    "print(next_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/f_rgwft14s14_j1xnpz_zn7c0000gn/T/ipykernel_13358/782265510.py:11: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  gp = SingleTaskGP(train_x, train_y)\n",
      "/Users/s/PhD/gitrepo/Torch-Practice/venv/lib/python3.13/site-packages/botorch/acquisition/analytic.py:331: NumericsWarning: ExpectedImprovement has known numerical issues that lead to suboptimal optimization performance. It is strongly recommended to simply replace\n",
      "\n",
      "\t ExpectedImprovement \t --> \t LogExpectedImprovement \n",
      "\n",
      "instead, which fixes the issues and has the same API. See https://arxiv.org/abs/2310.20708 for details.\n",
      "  legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "# Define some data points and a GP model\n",
    "train_x = torch.rand(10, 1)  # 10 data points, 1 feature each\n",
    "train_y = torch.sin(train_x)\n",
    "\n",
    "# Build a single-task GP model\n",
    "gp = SingleTaskGP(train_x, train_y)\n",
    "\n",
    "# Define an acquisition function (e.g., Expected Improvement)\n",
    "acqf = ExpectedImprovement(gp, best_f=train_y.max())\n",
    "\n",
    "# Specify the number of candidates to optimize over\n",
    "q = 1  # For single candidate optimization\n",
    "\n",
    "# Define the bounds of the search space (ensure correct shape: 2 x 1)\n",
    "bounds = torch.tensor([[0.], [1.]])  # Lower and upper bounds in the 1D space\n",
    "\n",
    "# Optimize the acquisition function to find the next sample\n",
    "next_point, _ = optimize_acqf(\n",
    "    acqf, \n",
    "    bounds=bounds,  # Pass bounds here\n",
    "    num_restarts=5,  # Number of random restarts for optimization\n",
    "    raw_samples=20,  # Number of raw candidates to sample before optimization\n",
    "    q=q  # Number of candidates to optimize over at once\n",
    ")\n",
    "\n",
    "print(next_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
